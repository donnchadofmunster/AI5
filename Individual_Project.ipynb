{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d6b817-fa39-43e7-a69c-f534f4303261",
   "metadata": {},
   "source": [
    "# Individual Project Submission for ENG5337\n",
    "Due date: 28th February 2025\n",
    "\n",
    "Artificial Intelligence (AI) and Machine Learning (ML) are revolutionizing the field of robotics. This project aims to provide hands-on experience in developing and evaluating AI/ML algorithms for a line-following robot using the Quanser Qbot simulated environment. In this project, students will consider the requirements for deploying AI/ML algorithms in a robotics environment and how to measure their performance.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Students should complete each section of this notbook. Notes:\n",
    "* Text enclosed in \"\"\"triple quotes\"\"\" should be replaced with the students own text.\n",
    "* The Python functions should be developed to integrate with the Quanser Python scripts. As such, they may need adjusted to run independently in this notebook. An example of this is shown in task A4.\n",
    "* In Section B2, plots should be embedded in the notebook as images - syntax for doing this is included below. Please remember to upload the relevant image files along with your submission. Failure to do so will result in loss of marks.\n",
    "\n",
    "Hello there son"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4a158-e539-4c82-8382-19e63892dac0",
   "metadata": {},
   "source": [
    "## Part A: Collecting Labelled Training Data\n",
    "\n",
    "### Input Data Choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec37568-ea7f-4dfa-92ae-4ff51915426d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### A1. Input Data Choice\n",
    "The downward facing camera is most suitable as the lines the robot follows are below it. \n",
    "Combination with LIDAR could allow the walls of the theatre to be used as reference points. LIDAR on the QBot detects in a 360 degree radius, giving angles and distances for objects relative to the robot. When used in conjunction with the Gyroscope, this could allow the QBot to work out its exact position relative to a specific cardinal direction, such as north. However, for the purposes of this task, the most efficient method would be using only the Downward Facing Camera. The downwards facing camera allows the robot to track the specific path its supposed to be following by checking whether or not the path is straight, left, right, or at a crossroads. This alone would be enough to ensure the robot stays on the line. When used in combination with the gyroscope, the robot could determine its optimal cardinal direction and use that to navigate crossroads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a7504-8432-48d3-af26-1a741b78005d",
   "metadata": {},
   "source": [
    "### A2. Data Pre-processing and Filtering\n",
    "Thresholding is the first pre-processing technique used to prepare the data. Done inside the vision.subselect_and_threshold function, relevent features are extracted by converting the image into binary, and selecting a maximum and minimum intensity value. This ensures that noise is also reduced as thresholding can be done to ensure that the line and only the line is visible, making sure that features such as noise are not treated as a followable line-path.\n",
    "\n",
    "The second techniue is segmentation. Segmentation is used inside the vision.image_find_objects method and returns blobs that fit the critereon for a line that could be followed. This allows allows the robot to track specific paths by recognising the white lines, allowing it to follow the line once the image has been thresholded to ensure that the robot does not chase after camera noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e574f6-58eb-4d6d-8205-485bbd08824b",
   "metadata": {},
   "source": [
    "### A3. Labelling Structure\n",
    "Data would likely be split into 5 sets: Straight, Left, Right, Crossroads, No-line.\n",
    "\n",
    "- Straight paths indicate that the robot needs to move straight forward.\n",
    "- If the line is on the left, the robot will have to turn left\n",
    "- Similarly, if the line is on the right, the robot will have to turn right\n",
    "- At a crossroads, the robot will have to make a decision as to which direction to take, as lines will be left, right and straight forward\n",
    "- Finally, if there are no lines, the robot is off track and should try and find a line to follow again\n",
    "\n",
    "These labels should cover every possible outcome for the downward facing camera, and so desicions will be made for each of these. Classification will be down based on thee position of the line. If the error between the line and the centre is between -25 and 25, the line will be considered Straight. Likewise, if the error is greater than 25, it will be determined to be a right turn and should it be less than -25, it will be labelled as a left turn.\n",
    "\n",
    "\n",
    "Experimentation will be done into how the robot reacts to crossroads once the model is implimented. For now, crossroads have not been added to the training data as it going straight forward is a valid decision at crossroads but for different routes with more specific paths, crossroads may indicate that the robot should turn left, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae7b42-9378-4837-a91c-47e1d1295684",
   "metadata": {},
   "source": [
    "### A4. Function to Collect, Label, and Store Training Data\n",
    "\n",
    "Copy and paste the python function you have written to collect, label and store training data for the line follwoing task in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898c81e-59fa-40b4-81a1-e9f98baa237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from pal.products.qbot_platform import QBotPlatformCSICamera\n",
    "from qbot_platform_functions import QBPVision\n",
    "\n",
    "DATA_DIR = \"training_data\"\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"training_data.csv\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "LABELS = [\"straight\", \"left\", \"right\", \"off_line\"]\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "for label in LABELS:\n",
    "    os.makedirs(os.path.join(DATA_DIR, label), exist_ok=True)\n",
    "\n",
    "def add_entry(image, label, error):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    img_filename = f\"{DATA_DIR}/{label}/img_{timestamp}.jpg\"\n",
    "    cv2.imwrite(img_filename, image)\n",
    "    df = pd.DataFrame([[img_filename, label, error]], columns=[\"image_path\", \"label\", \"error\"])\n",
    "    df.to_csv(CSV_PATH, mode='a', header=False, index=False)\n",
    "\n",
    "def collect_and_label_data(input):\n",
    "    # Your code here\n",
    "    vision = QBPVision()\n",
    "        \n",
    "    # Classificaiton\n",
    "    binary = vision.subselect_and_threshold(input, 50, 100, 225, 255) # Replace input with camera image for camera data\n",
    "    col, row, area = vision.image_find_objects(binary, 8, 500, 2000)\n",
    "    if col is None:\n",
    "        error = None\n",
    "        label = \"off_line\"\n",
    "        add_entry(binary, label, error)\n",
    "    else:\n",
    "        error = col - 160\n",
    "\n",
    "        if -25 <= error <= 25: # Straight Line\n",
    "            label = \"straight\"\n",
    "            add_entry(binary, label, error)\n",
    "        elif -200 < error < -25: # Left Turn\n",
    "            label = \"left\"\n",
    "            add_entry(binary, label, error)\n",
    "        elif 200 > error > 25: # Right Turn\n",
    "            label = \"right\"\n",
    "            add_entry(binary, label, error)\n",
    "\n",
    "# Insert test run of function HERE (see next cell for an example of how this might be achieved)\n",
    "\n",
    "def create_mock_input(bottom_x, top_x):\n",
    "    mock_image = np.zeros((200, 320), dtype=np.uint8)\n",
    "    cv2.line(mock_image, (top_x, 0), (bottom_x, 200), (255), 25)\n",
    "    return mock_image\n",
    "\n",
    "# Mock Data, to get real data could use camera_image\n",
    "for i in range(20):\n",
    "    x = random.randint(-60,380) # Out of bounds to simulate off line\n",
    "    x_offset = random.randint(-50,50) # Offset ensures not too steep turns for consistency with line\n",
    "    time.sleep(1)\n",
    "    collect_and_label_data(create_mock_input(x,x+x_offset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d5e47-5d89-42c2-be60-5aec6b3d038e",
   "metadata": {},
   "source": [
    "Make sure the function can run within this notebook. Below is an example of how this might be achieved by generating some artificial input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4f952d-8b74-4aae-b2fe-59b7539a26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "def save_data_example(image, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save binary image\n",
    "    image_filename = os.path.join(output_dir, \"image.png\")\n",
    "    cv2.imwrite(image_filename, image)\n",
    "\n",
    "# To run this function, we create dummy arguments\n",
    "\n",
    "# Create 50 x 320 array with entries of 0 and 1 randomly distributed\n",
    "array = np.random.randint(2, size=(50, 320))\n",
    "output_dir = r\"test_project\"\n",
    "\n",
    "# Test run of function\n",
    "# save_data_example(array, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19096754-a904-4984-857e-4f509d1b3747",
   "metadata": {},
   "source": [
    "## Part B: Performance Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfd842-c1cc-4762-95b4-be951d685b0a",
   "metadata": {},
   "source": [
    "### B1. Performance Metrics Functions\n",
    "In the following cell copy and paste the python function(s) you have written to measure the accuracy of the Quanser QBot in its line following task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43af7e31-bb58-4593-8b7c-2312b25ddb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Individual Project\n",
    "DATA_DIR = \"error_data\"\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"error_data.csv\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def add_error_metric(error):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    df = pd.DataFrame([[timestamp, error]], columns=[\"timestamp\", \"error\"])\n",
    "    df.to_csv(CSV_PATH, mode='a', header=False, index=False)\n",
    "\n",
    "def determine_error(input):\n",
    "    # Your code here\n",
    "    vision = QBPVision()\n",
    "        \n",
    "    # Classificaiton\n",
    "    binary = vision.subselect_and_threshold(input, 99, 101, 225, 255) # Replace input with camera image for camera data\n",
    "    col, row, area = vision.image_find_objects(binary, 8, 5, 200)\n",
    "    if col is None:  # Handle the case when no objects are found\n",
    "        error = 160\n",
    "    else:\n",
    "        error = abs(col - 160) # Maximum error = 160\n",
    "    return error\n",
    "\n",
    "def get_robot_image():\n",
    "    vision = QBPVision()\n",
    "    frameRate, sampleRate = 60.0, 1/60.0\n",
    "    downCam = QBotPlatformCSICamera(frameRate=frameRate, exposure = 39.0, gain=17.0)\n",
    "    undistorted = vision.df_camera_undistort(downCam.imageData)\n",
    "    camera_image = cv2.resize(undistorted, (320, 200))\n",
    "    return camera_image\n",
    "\n",
    "def analyse_robot_image(image):\n",
    "    add_error_metric(determine_error(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc14a3-b792-44f6-b45b-d5e283f489eb",
   "metadata": {},
   "source": [
    "### B2. Deployment of Performance Metrics Functions\n",
    "In the project descriptor, you were asked to measure accuracy of the line follower, that is how much the robot deviates from its intended path. The output of this function should be a plot which represents how the robot deviates from the line. For example, in the case of the down-cam sensor, you could calculate the distance of the centre of the robot from the centre of the white line (y-axis) in each frame (x-axis). Please enter the resulting plot (based on the performance of the standard line-following script provided in the Quanser Mobile Robotics downloads) in the cell below. Note to embed images in a Jupyter notebook you can use the following syntax (this will be explicitly visible in markdown mode):<img src=\"test1.jpg\" alt=\"Test Image\" width=\"300\"/>\n",
    "\n",
    "Remember to upload the image files along with your submission on Moodle! Failing to do so will result in loss of marks for this section!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971fed11-a823-479e-b895-57de24bcba33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid()\n\u001b[0;32m     15\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 17\u001b[0m \u001b[43mplot_error_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43merror_data/error_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m, in \u001b[0;36mplot_error_from_csv\u001b[1;34m(csv_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_error_from_csv\u001b[39m(csv_path):\n\u001b[1;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n\u001b[0;32m      5\u001b[0m     column_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      6\u001b[0m     rms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mmean(column_data\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_error_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    column_data = df.iloc[:, 1]\n",
    "    rms = np.sqrt(np.mean(column_data**2))\n",
    "    \n",
    "    plt.plot(df.iloc[:, 1], color = 'g', linestyle='-', label='Column 1')\n",
    "    plt.axhline(y=rms, color='mediumseagreen', linestyle='--', label=f'RMS: {rms:.2f}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Error Value')\n",
    "    plt.title('Robot Error')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_error_from_csv('error_data/error_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabf26f",
   "metadata": {},
   "source": [
    "The error can be seen by running the above code with the error_data provided from driving the robot automatically using the code from the previous labs. The absolute of the error was taken for this to account for both left and right errors, focussing primariy on deviation from the centre without regard to direction. The root mean squared was also provided to show the average error for the robot's path, useful when comparing how the robot does in comparison to other models when eventually this is used as part of the group project.\n",
    "\n",
    "![Error plot](error_plot.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
